% ---------------------------------------------------------------------- %
% TODO
%
% - [X] spelling and capitalization of
%   - [X] ocropy, OCRopus, <del>OCRopy</del>
%   - [X] kraken, Tesseract
%   - [X] calamari (OCR)
%   - [X] common API
% - [X] Table 1: Update GitHub Stars, SLOC and date checked
% - [X] Add URLs to models
% - [X] Add citations to OCR-D MP
% - [X] Plug ocropus/calamari comparison in calamari section
% - [X] revisit abstract
% - [X] revisit outline (last par in Introduction)
% - [ ] check the final numbering/referencing within sections
% - [X] fill in the ACM conf info
% - [X] fix over-wide tables
% - [X] fix bibtex warnings/errors
% - [X] Funding info
% - [X] Capitalize Tesseract
% - [ ] Remove unnecessary comments before packaging .tex sources for publication chair ;)
% - [ ] Submit :)
%
% ----------------------------------------------------------------------


\documentclass[sigconf]{acmart}
% \usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{multirow}

% https://tex.stackexchange.com/questions/346292/how-to-remove-conference-information-from-the-acm-2017-sigconf-template
\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column

\acmConference[HIP '19]{The 5th International Workshop on Historical Document Imaging and Processing}{September 20--21, 2019}{Sydney, Australia}

\begin{document}

\title{okralact - a multi-engine Open Source OCR training system}
\thanks{This work was partially supported by the Deutsche Forschungsgemeinschaft (DFG) - Project number 274863866}

\author{Konstantin Baierer}
\email{konstantin.baierer@sbb.spk-berlin.de}
\affiliation{%
    \institution{Staatsbibliothek zu Berlin \\ Preu\ss{}ischer Kulturbesitz}
}

\author{Rui Dong}
\email{dongrui@ccs.neu.edu}
\affiliation{\institution{Khoury College of Computer Sciences \\ Northeastern University}}

\author{Clemens Neudecker}
\email{clemens.neudecker@sbb.spk-berlin.de}
\affiliation{%
    \institution{Staatsbibliothek zu Berlin \\ Preu\ss{}ischer Kulturbesitz}
}

\begin{abstract}

Optical character recognition (OCR) of historical documents has
been significantly more difficult than OCR of modern texts
largely due to idiosyncrasies and wide variability of font,
layout, language, orthography of printed texts before ca.
1850. However, traditional OCR engines were optimized towards
supporting the widest possible set of modern text ("OmniFont
OCR") with little or no facilities for the user to adapt the
engine. Since OCR technologies began embracing deep neural
networks, various Free Software OCR engines are now available
that can in principle be adapted to different types of
documents by training specific models from ground truth (GT).
What these engines offer in terms of implementation finesse,
they lack in interoperability and standardization. To overcome
this, we developed okralact, a set of specifications and a
prototypical implementation of an engine-agnostic system for
training Open Source OCR engines like Tesseract, OCRopus,
kraken or Calamari. We discuss training of these engines, compare
their features, describe the specifications and functionality
of okralact and outline how a turn-key system for adapting
Open Source OCR engines can contribute to better OCR for
historical documents and to the general Open Source OCR
ecosystem.
\end{abstract}

\begin{CCSXML}
<ccs2012>
    <concept>
        <concept_id>10010405.10010497.10010504.10010508</concept_id>
        <concept_desc>Applied computing~Optical character recognition</concept_desc>
        <concept_significance>500</concept_significance>
    </concept>
    <concept>
        <concept_id>10010147.10010257.10010293.10010294</concept_id>
        <concept_desc>Computing methodologies~Neural networks</concept_desc>
        <concept_significance>300</concept_significance>
    </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Applied computing~Optical character recognition}
\ccsdesc[300]{Computing methodologies~Neural networks}

\maketitle

\section{Introduction}


Advances in deep learning have been a game-changer for image
processing, document layout analysis and text recognition, key components of optical
character recognition (OCR) technology. Text recognition in
particular has seen a paradigm-shift, away from a combination of character
segmentation and pattern-based classification, towards
segmentation-free recognition based on deep neural networks (NN).
% Since training from GT is so essential for the process, all NN-based OCR engines
% offer tools to train new models. However the usability of these training tools
% varies wildly. Different engines have different assumptions on the kind of input
% data they expect, the kind of hardware they are run on and employ slightly different
% terminology. The trained models are engine-specific, sometime even to a specific
% version of an engine and in most cases, it is very difficult to assess what
% kind of data a model was trained on after the fact.
%Any NN-based approach involves two steps: Training a model based on manually
%annotated ground truth (GT) and applying this model to data, in the case of
%OCR to recognize the text in images.

Most of the state-of-the-art OCR engines apply a hybrid recurrent convolutional
neural network combined with a connectionist temporal classification (CTC)
\cite{graves2006connectionist} layer as the OCR model. Deep convolutional
neural networks (CNN) \cite{krizhevsky2012imagenet} are added as the bottom
layers to extract hierarchical location-invariant features from the raw images
\cite{wick2018improving}. Recurrent neural networks (RNN)
\cite{mikolov2010recurrent}, which are effective in capturing the dependencies
in temporal sequences, are then added on top of the CNN layers to extract
sequential image representations. The CTC layer allows the text recognition
model to be trained on pairs of images and ground truth text without explicitly
aligning between them. It both saves the effort of segmenting the images into
characters and avoids possible errors introduced in this process.

In the particular context of historical documents and Fraktur fonts, e.g.
\cite{breuel2013high} have applied RNN with Long Short-Term Memory (LSTM) using OCRopus and obtained 
competitive accuracy when compared to ABBYY FineReader or Tesseract which in contrast to OCRopus also employ language modelling or adaptive techniques to post-process the raw output. 
Therefore, an additional benefit that comes with the application of LSTM is the reduced reliance on a specific language model or dictionary, both of which are typically sparse for historical languages, without sacrificing recognition accuracy \cite{ul2013can}.

%Any NN-based approach involves two steps: Training a model based on manually
%annotated ground truth (GT) and applying this model to data, in the case of
%OCR to recognize the text in images.

However, NN-based OCR engines vary wildly in their design
assumptions, e.g., the format of the input data, the hardware
requirements, as well as the set of adjustable parameters for
building and training an OCR model. For example, some engines can
only be run on CPU devices while others can tap into the
computation power of GPU devices. Some allow users to build their
own neural networks while others have a fixed model structure. The
trained models are engine-specific, sometimes even down to a specific
version of the engine and in most cases, it is very difficult to
assess what kind of data a model was trained on after the fact.  All of this has so far prevented the wider adoption and provision of NN-based 
OCR models for historical printed documents.

To overcome these obstacles to adoption, we propose
\textit{okralact}, a flexible set of specifications and a software
prototype to harmonize the input data, parameterization and
provenance tracking of training different OCR engines.
\textit{Okralact} is developed within the context of
OCR-D\footnote{http://ocr-d.de/}, a composition of a
coordination and several module projects established with support
from Deutsche Forschungsgemeinschaft (German Research Foundation,
DFG). OCR-D has the mission to examine available technologies for OCR and via the 
definition of standards and best practices in combination with the adaptation and development of state-of-the-art NN-based methods. OCR-D strives to improve the OCR process for mass-digitization of 
printed historical material \cite{neudecker2019datech}.  

The remainder of this paper is divided into three sections. Section
\ref{sec:ooocr} provides an overview of the currently most prominent Open Source OCR engines Tesseract, OCRopus, kraken and Calamari and compares their training features and models in greater detail. Section \ref{sec:okralact} introduces \textit{okralact} with its
specification and the describes the prototype implementation in more detail. We conclude by 
outlining remaining challenges and future work in section \ref{sec:conclusion}. 

\section{Open Source OCR}
\label{sec:ooocr}

There have been various Free Software projects that implement NN technologies
for OCR. For the scope of this work, we only consider the four most prominent projects: Tesseract, OCRopus, kraken and Calamari, all of which are released under Apache 2.0 license.

%\begin{table}[b]
%\begin{tabular}{llll}
%\hline
%Engine    & Main Developer     & Stack                      & Development \\ \hline
%OCRopus   & Tom Breuel         & Python                     & stale       \\
%kraken    & Benjamin Kiessling & Python, Torch              & active      \\
%Calamari  & Christoph Wick     & Python, Tensorflow         & active      \\
%Tesseract & Ray Smith          & C++                        & active
%\end{tabular}
%\caption{Basic properties of Open Source OCR engines}
%\label{tab:basic}
%\end{table}

\begin{table}[b]
\caption{Software metrics of selected Open Source OCR engines as of 2019-08-14}
\begin{tabular}{lllll}
\hline
Engine    & Contributors & First Release & GitHub Stars & SLOC \\ \hline
OCRopus   & 28           & 2010          & 2638         & 4996 \\
kraken    & 9            & 2015          & 174          & 4065 \\
Calamari  & 11           & 2018          & 377          & 6116 \\
Tesseract & 108          & 1985          & 29117        & 143251 \\

\end{tabular}
\label{tab:stats}
\end{table}

\subsection{Tesseract}

Tesseract \cite{4376991} was one of the first Free Software OCR
engines \cite{Rice1995TheFA} and remains one of the oldest OCR engines still in
development. Consistently helmed by Ray Smith, Tesseract has gone
through various iterations and partial rewrites and changing
ownership. Open Source since 2005 and sponsored by Google since
2006, Tesseract is the most well-known and most widely used Open
Source OCR engine (c.f. Table \ref{tab:stats}).

While earlier versions of Tesseract did include training facilities, 
these had severe limitations and required cumbersome data preparation and
character-based training. Attempts in training Tesseract for historical documents, e.g.
by the IMPACT \cite{PSNC} and eMOP \cite{doi:10.1093/llc/fqv062} projects, did not yield 
the desired improvements in OCR accuracy.

With the first alpha of version 4.0.0 released in 2016, Tesseract introduced a trainable
engine based on LSTM NN \cite{smith2016tesseract}. While still a very involved, multi-step process, the
improved accuracy and fine-tuning to specific corpora outweigh the effort.
There have been activities to streamline the training process, e.g.
ocrd-train\footnote{\url{https://github.com/OCR-D/ocrd-train}.} implements a training workflow for
Tesseract 4 as a Makefile.

\subsection{OCRopus}

The brainchild of Thomas Breuel and first released in 2007, OCRopus
\cite{breuel} was initially a set of tools around OCR based on
Tesseract. With the release of the Python implementation ocropy in
2010, OCRopus revolutionized OCR by adopting LSTM. OCRopus bundled a set of rudimentary but
usable command line tools not only to train and apply models, but
also to preprocess images (binarization, deskewing, layout analysis),
evaluate recognition results and offered a basic user interface to produce
GT data. Ocrocis is a noteable augmentation of OCRopus, providing best practices and
additional scripts and automation solutions for training \cite{springmann2015ocrocis}.

Breuel has since moved on to create clstm, a better
performing LSTM implementation \cite{DBLP:conf/icdar/Breuel17}, and
ocropy2, a rewrite of OCRopus based on CUDA and the PyTorch Deep
Learning framework \cite{DBLP:conf/icdar/Breuel17} but neither has
yet reached the same traction in the Free Software community as the
original Python version.


\subsection{kraken}

kraken \cite{DBLP:journals/corr/RomanovMSK17} started out as a fork of ocropy
by Benjamin Kiessling in 2015 to rectify a number of implementation and usability
issues while preserving (mostly) functional equivalence. While retaining many of
the algorithms around OCR, the actual recognition engine has been based on the
Torch machine learning framework since version 2.0 and can not reasonably be
considered a fork of ocropy anymore. kraken has been having a particular impact
on OCR of Arabic and is the technical foundation of the Open Islamic Texts
Initiative \cite{miller_romanov_savant_2018}.


\subsection{Calamari}

Christoph Wick has been developing the Calamari
\cite{DBLP:journals/corr/abs-1807-02004} OCR engine since 2018, based on the
codebases of ocropy and kraken but backed by the TensorFlow machine learning
framework. Calamari also added advanced features such as n-fold training and
voting algorithms \cite{wickcomparison} and is used as the main component of the OCR4all
framework.\footnote{\url{https://github.com/OCR4all}.}

\subsection{Feature comparison}

Different OCR engines display different design features in terms of the
technical details of implementation and the interfaces provided to the users.

\subsubsection{Implementation}

Tesseract is implemented in C++ while the
other three engines are all written in Python. Tesseract and ocropy
implement the neural networks as well as the inference from scratch without
utilizing any existing deep learning libraries. Kraken uses PyTorch as the
deep learning backend. Calamari is targeted at using different deep learning backends,
though currently only the TensorFlow backend is implemented.

\subsubsection{Hardware Requirements}

Both kraken and Calamari support GPU training, which offers significant
improvements with regard to computation speed compared to training on CPU devices.

\subsubsection{Model Parameters}


\begin{table}[b]
\caption{Model Design Options for Open Source OCR engines.}
\begin{tabular}{p{1cm}p{1.6cm}p{1cm}p{0.7cm}p{0.95cm}l}
\hline
Layers                   & Parameters   & OCRopus              & kraken   & Calamari & Tesseract \\ \hline
% CNN Layer& & NA & Yes & Yes & Yes  \\
\multirow{ 4}{*}{Input} & batch size& -& \checkmark & \checkmark   & \checkmark  \\
                        & height & \checkmark & \checkmark &  \checkmark & \checkmark  \\
                        & width & -& \checkmark  &  -  &\checkmark   \\ 
                        & channels & -&\checkmark  &  - & \checkmark  \\\hline
\multirow{ 3}{*}{\shortstack{Convolu-\\tional}}    & kernel size  & \multirow{3}{*}{NA} & \checkmark        & \checkmark        & \checkmark         \\
                        & activation   &                      & \checkmark        & -        & \checkmark         \\
                         & output size  &                      & \checkmark        & \checkmark        & \checkmark         \\ \hline
\multirow{2}{*}{Pooling} & kernel size  & \multirow{2}{*}{NA}  & \checkmark        & \checkmark        & \checkmark         \\
                         & stride size  &                      & \checkmark        & \checkmark       & -         \\ \hline
\multirow{2}{*}{Dropout} & probability  & \multirow{2}{*}{NA}  & \checkmark        & \checkmark        & \multirow{2}{*}{NA}         \\
                         & dimension    &                      & \checkmark        & -        &          \\ \hline
\multirow{5}{*}{Recurrent}     & hidden size  & \checkmark                    & \checkmark        & \checkmark        & \checkmark         \\
                         & direction    & \checkmark                    & \checkmark        & -        & \checkmark         \\
                         & dynamic cell & -                 & \checkmark          & -     & -  \\
                         & time axis    & -                    & \checkmark        & -        & \checkmark         \\
                         & summarize & - & \checkmark& -& \checkmark \\ \hline
Output                   & output size  & -                    & \checkmark        & -        & \checkmark         \\
\end{tabular}
\label{tab:model_param1}
\end{table}

Table~\ref{tab:model_param1} illustrates that different OCR engines provide
different levels of freedom for the users to design their own neural network
structures. Both kraken and Tesseract use Variable-size Graph Specification Language (VGSL) to specify the network architecture, while Calamari has its own model description language. 
OCRopus provides the fewest parameters to define the model structure and only supports recurrent layer. The raw image features are first rescaled according to the user-specified line height and then directly fed into a fixed number of LSTM layers. It only allows users to choose whether to use bidirectional or unidirectional LSTM layers and the size of hidden states. Tesseract, kraken and Calamari allow users to define the shape of the input and add arbitrary numbers of convolutional, max pooling \cite{boureau2010theoretical} and recurrent layers. Calamari and kraken also support dropout layers. 

The input layer definition is required for Tesseract and kraken but optional for Calamari. The user can define the mini-batch size for those three engines. For Tesseract and kraken, the input image will be scaled with the user-specific height and width if non-zero values are set for those parameters. Calamari will scale the image according to the user specified height. Tesseract and kraken can take both grayscale and RGB color images as input.

Convolutional and max pooling layers are used to extract image features to be fed into RNN layers. All three engines support user-specified kernel size and output size in the convolutional layer, but Tesseract and kraken also provide additional options for activation functions. The max pooling layer is used after the convolutional layer to down-sample the image features and keep the sharpest ones. Calamari and kraken allow the users to define both the kernel and stride size for pooling layer, while Tesseract only supports user-specific kernel size.

Dropout layers are used to prevent overfitting on training data by randomly dropping out units from the neural network. Users can specify the dropout probability for both Calamari and kraken but only decide dropout dimension, i.e., whether to apply dropout on 1 dimension or 2 dimensions, for kraken. 

Calamari only supports bidirectional recurrent layers, while Tesseract and kraken allow forward, backward or bidirectional recurrent layers. Users can choose to use LSTM or GRU cells for kraken, while only LSTM cells are possible with Tesseract and Calamari. Furthermore, kraken and Tesseract support a user-specific temporal dimension, and allow summarizing the output of recurrent layer in that dimension.

Users can also specify the number of output classes in the output layer with kraken and Tesseract. In the current implementation, however, both of them would just ignore the user-specified number and automatically obtain it from the training data. 
%Kraken allows user to choose different ways to resize the number of classes only when fine tuning an existing model with new training data. 

There are some other engine-specific layers not included in the table. For example, Tesseract allows the user to define a fully connected layer, where the user can define the non-linearity activation function and output size. It also supports a rescale layer to rescale a 2 dimensional output to a 3 dimensional one before feeding it into the next layer. Kraken supports a reshape layer to remove undesirable non-1 height before a recurrent layer.
%The kernel size is the size of the sub-region for pooling and the stride size is the number of pixels to move the filter across the image in vertical and horizontal directions.  

\subsubsection{Training Parameters}

\begin{table}[b]
\caption{Training Options for Open Source OCR engines.}
\begin{tabular}{llp{1cm}llp{1cm}}
\hline
Parameters                     & OCRopus & kraken & Calamari & Tesseract \\ \hline
% CNN Layer                      &         & NA      & Yes    & Yes      & Yes       \\
Learning Rate &  \checkmark       & \checkmark      & \checkmark      &  \checkmark        \\
\hline
Optimizer                      &      -       & \checkmark      & \checkmark         & \checkmark         \\ \hline
Training time                   &      \checkmark      & \checkmark      & \checkmark & \checkmark         \\\hline
Early Stop                     &      -       & \checkmark      & \checkmark        & \checkmark         \\
%\hline
% Maximum &           & lines & epoches                 & iterations               & iterations   \\
%         & condition &       & performance not         & performance continuously & error rate   \\
%         &           &       & significantly improving & getting worse            & small enough \\ \hline
\end{tabular}
\label{tab:training_options}
\end{table}

The four OCR engines provide different levels of flexibility to fine-tune training
parameters such as the learning rate, which optimizer to use or when to stop training
as summarized in Table~\ref{tab:training_options}. 

All four engines allow user to specify learning rate, while Tesseract is the only one that allows different layers to have their own learning rates. Both Calamari and Tesseract allow the user to choose between Adam  \cite{kingma2014adam} and Momentum optimizer \cite{QIAN1999145}, while kraken supports Adam, Stochastic Gradient Descent (SGD) \cite{bottou2010large} and RMSprop \cite{hinton2012neural}. For different optimizers, different OCR engines also provide different ways to adjust
optimizer parameters. 

All four engines allow the user to specify how long the model should be trained in terms of maximum number of training samples, epochs or iterations. Kraken, Calamari and Tesseract also support early stopping, i.e., stopping training earlier than the specified ending point, when the model
performance satisfies some given conditions. For example, kraken supports early
stopping when the model performance improvement is not significant anymore. Tesseract
stops training when the error rate is lower than a user-specific target error
rate. Calamari stops training when the model performance is continuously
getting worse.

\subsubsection{Fine Tuning}

All four engines provide a user interface for fine-tuning an
existing model with new training data. OCRopus and Calamari only support
continued training of an existing model with new data samples,
while kraken and Tesseract also allow the user to modify the structure of an existing model. Users can cut off the top of the network at a given layer and append a new model structure after it.
For example, to train an OCR model for a new
language which shares many characters with an existing model,
the bottom convolutional layers can be reused to extract similar image features. And the recurrent layers can be retrained to capture the new dependency among the image features.

\subsubsection{Tooling / User Interfaces}

All examined OCR engines provide basic user interfaces to
train OCR models, although they vary in the set of adjustable
parameters and ease of use. Calamari and kraken, due to their lineage,
provide some tools similar to ocropy or advise to use ocropy's tools
directly. The ocropy codebase is not in active development hence the interfaces
are frozen in the state of ca. 2015, also to avoid breaking legacy OCR
workflows. Both kraken and Calamari significantly improve the command
line interfaces. While kraken allows for elegant and terse calls by
removing configurability of obsolete or little-used features, Calamari
exposes as much of the power to the user, resulting in a long list
of parameters.

Training Tesseract is significantly more complex than training the other three engines. 
OCRopus, kraken and Calamari all require a single command line call to train
a model, while the process with Tesseract is more involved.
For example, Tesseract requires the generation of bounding boxes for each text 
line although it does not use individual character coordinates. 
The character set is then explicitly generated from the bounding box files by the
command ``\textit{unicharset\_extractor}''. Training data are
provided to the training interface via ``lstmf'' files, where each
file is serialized document data containing an image and the
corresponding GT text. These files are generated via the
command ``\textit{Tesseract}''. Then a traineddata file is generated
to obtain all the information it needs on the language to be
learned via the ``\textit{combine\_lang\_model}'' command. Finally,
command ``\textit{lstmtraining}'' is used to train an OCR model with
the traineddata file and the list of lstmf files as input.

\subsubsection{Standard models}
\label{sec:models}

A well-engineered OCR engine is only the technical foundation for
good recognition results. How accurately an OCR engine recognizes text
in a particular document crucially depends on how the model was trained. In
an ideal world of unlimited resources, technically proficient domain experts
would transcribe corpus- or even document-specific GT and use a system such as
\textit{okralact} or the training facilities of individual engines to train
tailor-made models for optimal recognition accuracy. In reality, due to lack of
resources to transcribe GT and train models as well as the technical
challenges involved, the availability and quality of ready-to-use models is 
essential to the quality of an OCR engine as perceived by the bulk of users.

Tesseract has the widest range of pre-trained models available of the engines
supported by \textit{okralact}. Trained on synthetic data, there are models available
for a wide variety of languages and scripts, including Fraktur. Since the shift
to NN-based recognition, two tiers of models are available, tessdata\_best and
tessdata\_fast,\footnote{\url{https://github.com/tesseract-ocr/tessdata_best} and \url{https://github.com/tesseract-ocr/tessdata_fast} resp.} with the latter trading off some accuracy for recognition speed.

The default model provided for OCRopus by Tom Breuel in 2015\footnote{\url{http://www.tmbdev.net/ocropy/}.} has been trained on
both synthetic and real GT and has been widely used in both ocropy and kraken.
Breuel also offers a model for recognition of Fraktur. The OCRopus community
produced models for Old French,\footnote{\url{https://github.com/zuphilip/ocropy-french-models}.} Polytonic Greek\footnote{\url{https://github.com/brobertson/ciaconna/tree/master/Classifiers}.}, Japanese\footnote{\url{https://github.com/isaomatsunami/clstm-Japanese}.} and many more.\footnote{\url{https://github.com/tmbdev/ocropy/wiki/Models}.}
Kraken integrates the widest variety of models and model formats,
supporting not only ocropy and clstm models but also a more efficient
protobuf-based encoding of ocropy models and, since version 2.0, CoreML-encoded
models. As part of the architecture, models can be loaded from a GitHub-based
model repository, offering models for Arabic, Polytonic Greek and German Fraktur.\footnote{\url{https://github.com/mittagesen/kraken-models}.}
Calamari supports only its proprietary model format but uses the same approach
to model distribution as kraken, offering a GitHub repository with pretrained
models, mostly targeted towards German in both Antiqua and Fraktur scripts.\footnote{\url{https://github.com/Calamari-OCR/calamari_models}.}
Tesseract is the only project that publishes the GT the standard models are
trained on.\footnote{\url{https://github.com/tesseract-ocr/langdata_lstm}.} In general, provenance information on GT, model structure and training 
parameters is not provided for the standard models of all engines.
The kraken models are described by a very basic JSON metadata format but not
deeply enough to recreate the models. Adapting the specifications outlined
in section \ref{sec:specs}, e.g. by training with the okralact implementation
described in \ref{sec:prototype}, full provenance information can be retained,
making the models more widely usable and sustainable.


\section{okralact}
\label{sec:okralact}


\begin{figure*}[ht!]
    \begin{center}
        \includegraphics[width=1\linewidth]{Figures/Framework.png}
    \end{center}
    \caption{Framework of the \textit{okralact} system.}
    \label{fig:framework}
\end{figure*}

\textit{Okralact} is both a set of specifications and a prototype
implementation for harmonizing the input data, parameterization and
provenance tracking of training different OCR engines.

\subsection{Specifications}
\label{sec:specs}

\textit{Okralact} is an integral part of the OCR-D project that has been
consolidating specifications and Open Source software around OCR
with a focus on enabling mass full text digitization of historical
printed documents.\footnote{\url{https://ocr-d.github.io}.} As with the
OCR-D project proper, the primary focus of \textit{okralact} is to develop
specifications, technical documentation about the different engines
and their parameterization. These specifications are made
actionable in a prototype training infrastructure (c.f.
\ref{sec:prototype}) but are developed with different usage
scenarios as well as the upstream engine developers in mind.

First, we propose a container format for bundling OCR GT data.
While OCR engines expect GT in the form of line-wise image-text
tuples and supported file formats, encodings, file structure and
naming vary from engine to engine. The engines operate on image and
text data exclusively without the need for layout information.
Therefore, no engine currently supports any of the common OCR file
formats (ALTO, hOCR, PAGE-XML) as input. We aim to change this by
reusing the OCRD-ZIP GT container format developed
within OCR-D \footnote{\url{https://ocr-d.github.io/ocrd_zip}.}
\cite{boenig2019datech}, that supports all OCR formats as payload but is
particularly geared towards PAGE-XML. This will allow users to apply advanced
tools for GT creation, such as PRImA Research Labs' Aletheia
\cite{clausner2011aletheia}.

Secondly, we compared the parameterization of the tools and
identified a set of common parameters that are equivalent across
the engines, yet may differ slightly in their usage or terminology. We unified the language of defining the neural network structure and built an automatic translator for the unified language to the engine-specific language. Therefore users do not need to learn the modeling language of different engines by reading documentation or diving into the source code. We also unified some shared training parameters.
For example, we unified the definition of training time in epochs and convert it to number of lines and iterations according to the data size and the user specified batch size for each engine.
Beyond this Common API, there are various configuration options and
parameters that are unique per engine. We gathered this information
in a JSON Schema document that defines the Common API and
engine-specific parameters in machine-actionable
form.\footnote{\url{https://ocr-d.github.io/gt-profile.yaml}.} The
schema can then be used to validate input configurations to the
training engine or to generate documentation such as help pages,
user interface hints and auto-completion.

Lastly, we look at how trained models are distributed and
define a BagIt \cite{kunze2018bagit} profile for describing trained models. 
This format allows bundling the (engine-specific) models with the 
configuration used to train it and optionally the
GT that the model was based upon. This supports the reuse of models 
since it combines the provenance of the model in terms of GT and 
parameterization used, as well as  intermediary evaluation results and 
log files.
% * OCRD-ZIP GT format (shortly)
% * JSON schema for parameters / Common API
% * https://github.com/OCR-D/spec/pull/105/files
%     * Drop the line GT part
%     * Clean up evaluation



\subsection{Prototype}
\label{sec:prototype}

As shown in Figure \ref{fig:framework}, \textit{okralact} is a client/server
architecture application. The interactions between the client nodes and the
server are implemented using Flask \footnote{\url{http://flask.pocoo.org/}}, a lightweight web application framework for Python.
Users can upload training or evaluation datasets and configuration files
specifying the OCR engine(s) with the corresponding parameters to be used. 
The user can also submit a request of training an OCR model with the selected 
dataset and configuration file. After the job is submitted, the user can check the job
progress. Once the training of a model is complete, it can be downloaded together with a report summarizing the performance of each intermediate model.
The user can also submit an evaluation request with an evaluation dataset and
a selected model and receive an evaluation report for the OCR results.

% All these communication between server and client nodes are made through HTTP methods responded by Flask routes.  

% kba: Rui what does the user receive for the evaluation request?
% CN: Looks like currently Levenshtein distance is calculated, for
% future work bag-of-words might be a useful addition (segmentation
% errors)

% kba: We should briefly explain how we run the evaluation (not rely on the numbers by the engines but regularly recognize with snapshot models, calculate adapted levenshtein for CER, considering WER or BOW WER).

Since training and evaluation are both long-running process, HTTP communication must be asynchronous,
i.e., all the training or evaluation
jobs submitted to the server are handled in the background by task queues. \textit{okralact} uses the popular task queue Redis Queue \footnote{\url{https://python-rq.org/}} (RQ),
a Python library backed by Redis that
executes the work outside an HTTP request-response cycle. The application puts job
requests into the queue of a given worker process, which serves in
first-in-first-out order. When a job gets the opportunity to run, it performs a call to the common
API for different OCR engines to interpret the configuration files into the
training or evaluation command for the specified OCR engine, and then executes the training command. The application on the server communicates with the RQ workers via Redis message queue. After submitting a job, the application can monitor the job progress by interacting with the message queues. \textit{okralact} has two worker processes in the background, one for training jobs and one for evaluation jobs, so that training and evaluation jobs can run simultaneously and independently on the same server.

It is worth noting that \textit{okralact} has its own component for generating an evaluation report. Different engines provide evaluation reports with different levels of detail, e.g. kraken provides a very detailed report about the character error rate (CER) and the confusion matrix, while ocropy has a command to compute confusion matrices. All engines produce variations of CER in the log output. To provide a unified evaluation report, \textit{okralact} first generates an OCR result for each image and then evaluates the result in terms of CER by computing Levenshtein distance between the OCR result and the ground truth. We will add other evaluation metrics such as word error rate (WER) and confusion matrix later. WER can provide a rough estimation for the performance of downstream tasks such as information retrieval (IR) or named entity recognition (NER). A confusion matrix can provide better understanding of OCR errors, i.e., the frequency at which the engines make particular errors.

\section{Conclusion and Future Work}
\label{sec:conclusion}

With the current state of \textit{okralact}, it is possible to describe the
training process in a standardized way and to specify the most important
aspects  of engine configuration with the Common API, and any engine-specific
features with custom parameters.


Though we largely succeeded in harmonizing the majority of engine APIs into the
\textit{okralact} Common API, there still remain many engine-specific parameters. Some
features, like Calamari's n-fold training or Tesseract's advanced language
modelling, are too engine-specific to be encapsulated into the Common API. 
For those features, users can augment the Common API with engine-specific parameters, 
enabling power users to utilize the full range of engine capabilities while still requiring 
next to no knowledge of engine implementation details from less technically inclined users.

We want to encourage engine developers and
users to enter into discussions about unified terminology, data types and exchange 
formats that can help improve engine interoperability and further extend the range of the Common API to
make \textit{okralact} and the engines it supports even more
accessible to non-expert users.

The next steps in \textit{okralact} development include extending, documenting and standardizing
the evaluation of models, better support for transfer learning \cite{027.7169} and extending the provenance-tracking exchange format for trained models with a focus on describing not only the models produced by \textit{okralact} but
also existing models produced by the community such as those mentioned in
section~\ref{sec:models}. The \textit{okralact} prototype is Free Software, the
specifications are in the public domain, development will continue openly on
GitHub and is open to outside collaborators.\footnote{\url{https://github.com/OCR-D/okralact}.}
% Cleanly defined evaluation measures necessary (no more ISRI or character-counting CER)
% Furthermore, there is a sore lack of easy to use tools with well
% defined measures for the evaluation of OCR. Traditionally, the
% character error rate (CER) and to a lesser extent word error rate
% (WER) have been the metrics to measure accuracy. However, these
% measures require perfect GT and results can be misleading for
% breakdowns in preprocessing such as segmentation errors or
% technical flaws in model preparation such as characters not part
% of the encoding.
% see prototype
% CN: At least within IAPR, CER/WER is still the most prominent and
% undisputed metric. One could perhaps argue for consideration of
% ReadingOrder in the evaluation, for necessary normalizations of
% encodings, how to deal with (and which) stopwords, asf. - but I
% am not really aware of any concerted efforts in this direction?
% TBH, the whole claim made here that evaluation methods and
% metrics are a challenge in this context is not yet entirely clear
% to me?
% More flexible choice of input data (multiple works, individual lines from individual pages etc.)
% Integration of training workflow and recognition workflows
Finally, we aim for a better integration of training and
recognition workflows. As \textit{okralact} is part of the OCR-D
ecosphere, which has been developing components of a full-stack
OCR workflow,\footnote{Visit \url{https://ocr-d.github.io/projects} for a list of all module projects of OCR-D.} 
%including image preprocessing,\footnote{\url{https://github.com/syedsaqibbukhari/docanalysis}.}, layout detection,\footnote{\url{https://github.com/ocr-d-modul-2-segmentierung}.}\footnote{\url{https://github.com/mjenckel/LAYoutERkennung}.} \cite{Reul:2017:LSO:3078081.3078097}, font detection,\footnote{\url{https://github.com/seuretm/ocrd_typegroups_classifier}.} layout classification, recognition and language-model post-processing \cite{englmaier2019datech}, 
we intend to integrate \textit{okralact} into the
OCR-D specification and reference implementation. This will enable
users to pick and choose the best Free Software implementations and models for
the whole gamut of text recognition needs for historic documents.

\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography}

\end{document}
